# CLCAE (Official PyTorch Implementation)

![Python 3.8](https://img.shields.io/badge/python-3.8-green.svg)![License](https://img.shields.io/badge/license-MIT-yellow)


### [Arxiv](https://arxiv.org/abs/2211.11448)

**Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint, CVPR2023**

[Hongyu Liu](https://github.com/KumapowerLIU)<sup>1</sup>,
[Yibing Song](https://ybsong00.github.io/)<sup>2</sup>,
[Qifeng Chen](https://cqf.io/)<sup>3</sup>

<sup>1</sup>HKUST, <sup>2</sup> AI$^3$ Institute, Fudan University, <sup>3</sup>HKUST

<img src='doc/teaser.png'>



## :sparkles: Pipeline

### [How to find the suitable foundation latent code $w$? Using Contrastive Learning!]
<img src='doc/contrastive.png'>

### [Obtaining the better latent code $w^+$ and $f$ based on foundation latent code $w$! ]
<img src='doc/contrastive.png'>


## Citation

If you find our work useful for your research, please consider citing the following papers :)

```bibtex
@article{liu2022delving,
  title={Delving StyleGAN Inversion for Image Editing: A Foundation Latent Space Viewpoint},
  author={Liu, Hongyu and Song, Yibing and Chen, Qifeng},
  journal={arXiv preprint arXiv:2211.11448},
  year={2022}
}
```


## License

The codes and the pretrained model in this repository are under the MIT license as specified by the LICENSE file.